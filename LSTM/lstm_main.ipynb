{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "train_size = 0.8\n",
    "embedding_vector_len = 100\n",
    "max_words = 5000\n",
    "epochs = 10\n",
    "batch_size=50\n",
    "source_folder = \"gutenberg_texts\"\n",
    "files_to_read = 4\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads data from one file\n",
    "#with open(\"gutenberg_texts\\pg21687.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#   data = f.read().lower()\n",
    "\n",
    "data_list = []\n",
    "count = 0\n",
    "for file_name in os.listdir(source_folder):\n",
    "    if count == files_to_read:\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "    try:\n",
    "        file_path = os.path.join(source_folder, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read().lower()\n",
    "        data_list.append(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with reading {file_name}: {e}\")\n",
    "\n",
    "data_str = \" \".join(data_list)\n",
    "tokenizer = Tokenizer(char_level=False, filters=\"!\\\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~\", num_words=max_words)\n",
    "tokenizer.fit_on_texts([data_str])\n",
    "#tokenized_data = word_tokenize(data.lower())\n",
    "sequences = tokenizer.texts_to_sequences([data_str])[0]\n",
    "#print(sequences[:20])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "#print(sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315158, 50)\n"
     ]
    }
   ],
   "source": [
    "#splitting up text\n",
    "seq_arr = []\n",
    "\n",
    "for i in range(len(sequences) - seq_len):\n",
    "    seq_arr.append(sequences[i:i + seq_len])\n",
    "\n",
    "padded_seq = pad_sequences(seq_arr, maxlen = seq_len)\n",
    "\n",
    "print(padded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 252126 X_test: 63032\n",
      "(252126,)\n"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "train_partition = int(len(padded_seq) * train_size)\n",
    "\n",
    "X_train, X_test = padded_seq[:train_partition], padded_seq[train_partition:]\n",
    "y_train, y_test = sequences[seq_len: train_partition + seq_len], sequences[train_partition + seq_len:]\n",
    "#y_train = to_categorical(y_train, num_classes=vocab_size)\n",
    "#y_test = to_categorical(y_test, num_classes=vocab_size)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"X_train: {len(X_train)} X_test: {len(X_test)}\")\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_vector_len, input_length = seq_len),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    LSTM(150),\n",
    "    Dropout(0.4),\n",
    "    Dense(vocab_size, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2133s\u001b[0m 270ms/step - accuracy: 0.0529 - loss: 6.7251\n",
      "Epoch 2/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2166s\u001b[0m 275ms/step - accuracy: 0.0827 - loss: 5.9963\n",
      "Epoch 3/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2219s\u001b[0m 282ms/step - accuracy: 0.1087 - loss: 5.6852\n",
      "Epoch 4/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2327s\u001b[0m 295ms/step - accuracy: 0.1291 - loss: 5.4644\n",
      "Epoch 5/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1972s\u001b[0m 250ms/step - accuracy: 0.1405 - loss: 5.2995\n",
      "Epoch 6/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2010s\u001b[0m 255ms/step - accuracy: 0.1494 - loss: 5.1827\n",
      "Epoch 7/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2170s\u001b[0m 275ms/step - accuracy: 0.1542 - loss: 5.0839\n",
      "Epoch 8/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2107s\u001b[0m 267ms/step - accuracy: 0.1608 - loss: 4.9994\n",
      "Epoch 9/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2237s\u001b[0m 284ms/step - accuracy: 0.1664 - loss: 4.9162\n",
      "Epoch 10/10\n",
      "\u001b[1m7879/7879\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2124s\u001b[0m 270ms/step - accuracy: 0.1710 - loss: 4.8473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(X_train, y_train, epochs=epochs, verbose=1)\n",
    "model.save(\"lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#generating story\\nstory_len = 0\\n#while loop for starting input\\nwhile True:\\n    start_text = input(\"How do you want to start your story?\")\\n    if isinstance(start_text, str):\\n        break\\n    elif len(list(start_text.split())) > 50:\\n        print(\"The beginning of your story should be 50 words or less\")\\n    else:\\n        print(\"Enter a valid beginning\")\\n\\n#while loop for story length\\nwhile True:\\n    len_choice = input(\"How many words do you want in your story?\")\\n    if not len_choice.isdigit():\\n        print(\"Please enter a valid number\")\\n    elif int(len_choice) < 0:\\n        print(\"Please enter a number greater than 0\")\\n    else:\\n        story_len = int(len_choice)\\n        break\\n\\nprint(\"Proceeding to your story...\")\\nprint(\"Start text:\", start_text)\\nfor word in start_text.split():\\n    if word not in tokenizer.word_index:\\n        print(f\"Word \\'{word}\\' is not in the vocabulary.\")\\n\\ngenerated_story = start_text\\n\\n#preprocessing starting input\\ntokenized_start = tokenizer.texts_to_sequences([start_text.lower()])\\nprint(\"Tokenized start:\", tokenized_start)\\n\\nif not tokenized_start or len(tokenized_start[0]) == 0:\\n    print(\"No valid tokens in start text.\")\\nelse:\\n    start_sequence = pad_sequences(tokenized_start, maxlen=seq_len, padding=\"pre\")\\n\\nstart_sequence = pad_sequences(tokenized_start, maxlen=seq_len, padding=\"pre\")\\n\\nprint(type(start_sequence))\\nprint(f\"Start sequence shape: {start_sequence.shape}\")\\nprint(f\"Start sequence (first entry): {start_sequence[0]}\")\\n\\n#try statement for generating text\\nfor i in range(story_len):\\n    try:\\n        pred_text = model.predict(start_sequence, verbose=0)\\n        #print(type(text))\\n        #print(text[0])\\n\\n        preds = np.asarray(pred_text).astype(\"float64\")\\n        preds = np.log(preds + 1e-7) / temperature\\n\\n        #pred_text_index = np.argmax(preds, axis=-1)[0]\\n        exp_preds = np.exp(preds)\\n        preds = exp_preds / np.sum(exp_preds)\\n\\n        print(\"Predicted probabilities:\", preds)\\n\\n\\n        pred_text_index = np.random.choice(len(preds[0]), p=preds[0])\\n\\n        #print(f\"Predictions: {pred_text[0]}\")\\n\\n        pred_word = tokenizer.index_word.get(pred_text_index, \"Unknown\")\\n        generated_story+=\" \" + pred_word\\n\\n        print(pred_word)\\n\\n        start_sequence = pad_sequences([start_sequence[0].tolist() + [pred_text_index]], maxlen=seq_len, padding=\"pre\")\\n    except Exception as e:\\n        print(f\"Error with generating text: {e}\")'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#generating story\n",
    "story_len = 0\n",
    "#while loop for starting input\n",
    "while True:\n",
    "    start_text = input(\"How do you want to start your story?\")\n",
    "    if isinstance(start_text, str):\n",
    "        break\n",
    "    elif len(list(start_text.split())) > 50:\n",
    "        print(\"The beginning of your story should be 50 words or less\")\n",
    "    else:\n",
    "        print(\"Enter a valid beginning\")\n",
    "\n",
    "#while loop for story length\n",
    "while True:\n",
    "    len_choice = input(\"How many words do you want in your story?\")\n",
    "    if not len_choice.isdigit():\n",
    "        print(\"Please enter a valid number\")\n",
    "    elif int(len_choice) < 0:\n",
    "        print(\"Please enter a number greater than 0\")\n",
    "    else:\n",
    "        story_len = int(len_choice)\n",
    "        break\n",
    "\n",
    "print(\"Proceeding to your story...\")\n",
    "print(\"Start text:\", start_text)\n",
    "for word in start_text.split():\n",
    "    if word not in tokenizer.word_index:\n",
    "        print(f\"Word '{word}' is not in the vocabulary.\")\n",
    "\n",
    "generated_story = start_text\n",
    "\n",
    "#preprocessing starting input\n",
    "tokenized_start = tokenizer.texts_to_sequences([start_text.lower()])\n",
    "print(\"Tokenized start:\", tokenized_start)\n",
    "\n",
    "if not tokenized_start or len(tokenized_start[0]) == 0:\n",
    "    print(\"No valid tokens in start text.\")\n",
    "else:\n",
    "    start_sequence = pad_sequences(tokenized_start, maxlen=seq_len, padding=\"pre\")\n",
    "\n",
    "start_sequence = pad_sequences(tokenized_start, maxlen=seq_len, padding=\"pre\")\n",
    "\n",
    "print(type(start_sequence))\n",
    "print(f\"Start sequence shape: {start_sequence.shape}\")\n",
    "print(f\"Start sequence (first entry): {start_sequence[0]}\")\n",
    "\n",
    "#try statement for generating text\n",
    "for i in range(story_len):\n",
    "    try:\n",
    "        pred_text = model.predict(start_sequence, verbose=0)\n",
    "        #print(type(text))\n",
    "        #print(text[0])\n",
    "\n",
    "        preds = np.asarray(pred_text).astype(\"float64\")\n",
    "        preds = np.log(preds + 1e-7) / temperature\n",
    "\n",
    "        #pred_text_index = np.argmax(preds, axis=-1)[0]\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        print(\"Predicted probabilities:\", preds)\n",
    "\n",
    "\n",
    "        pred_text_index = np.random.choice(len(preds[0]), p=preds[0])\n",
    "\n",
    "        #print(f\"Predictions: {pred_text[0]}\")\n",
    "\n",
    "        pred_word = tokenizer.index_word.get(pred_text_index, \"Unknown\")\n",
    "        generated_story+=\" \" + pred_word\n",
    "\n",
    "        print(pred_word)\n",
    "\n",
    "        start_sequence = pad_sequences([start_sequence[0].tolist() + [pred_text_index]], maxlen=seq_len, padding=\"pre\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with generating text: {e}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generated_story)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
