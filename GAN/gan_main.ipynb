{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "image_list = []\n",
    "source_folder = \"processed_gallery\"\n",
    "size = (128, 128)\n",
    "shape_size = (128, 128, 3)\n",
    "latent_dim = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "epochs = 5000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images from folder\n",
    "for file_name in os.listdir(source_folder):\n",
    "    try:\n",
    "        image = cv2.imread(os.path.join(source_folder, file_name))\n",
    "\n",
    "        image_list.append(image.astype(np.float32) / 255.0)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with loading image {os.path.join(source_folder, file_name)}: {e}\")\n",
    "\n",
    "data = np.array(image_list)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.shuffle(buffer_size=1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        layers.Dense(8 * 8 * 512, activation=\"relu\", input_dim=latent_dim),\n",
    "        layers.Reshape((8, 8, 512)),\n",
    "\n",
    "        layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding=\"same\", activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\", activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", activation=\"tanh\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\", input_shape=shape_size),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(512, (5, 5), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(negative_slope=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "#saving images\n",
    "def save_results(generator, epoch, test_data):\n",
    "    predictions = generator(test_data, training=False)\n",
    "    predictions = (predictions * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "    dest = \"results_gallery\"\n",
    "\n",
    "    for i, img in enumerate(predictions):\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        file_path = os.path.join(dest, f\"epoch_{epoch}_image_{i}.jpg\")\n",
    "\n",
    "        cv2.imwrite(file_path, img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "def discriminator_loss(real_out, fake_out):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_out), real_out)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_out), fake_out)\n",
    "    loss = real_loss + fake_loss\n",
    "    return loss\n",
    "def generator_loss(fake_out):\n",
    "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_out), fake_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "optimizer_G = Adam(learning_rate=lr, beta_1=beta1, beta_2=beta2)\n",
    "optimizer_D = Adam(learning_rate=lr, beta_1=beta1, beta_2=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for real in dataset.batch(batch_size):\n",
    "\n",
    "        real = tf.reshape(real, (-1, 128, 128, 3))\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        noise = tf.convert_to_tensor(noise, dtype=tf.float32)\n",
    "#train discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            fake = generator(noise, training=True)\n",
    "            real_result = discriminator(real, training=True)\n",
    "            fake_result = discriminator(fake, training=True)\n",
    "\n",
    "            disc_loss = discriminator_loss(real_result, fake_result)\n",
    "\n",
    "        disc_gradient = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        optimizer_D.apply_gradients(zip(disc_gradient, discriminator.trainable_variables))\n",
    "\n",
    "#train generator\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            new_fakes = generator(noise, training=True)\n",
    "            fake_result = discriminator(new_fakes, training=True)\n",
    "\n",
    "            gen_loss = generator_loss(fake_result)\n",
    "\n",
    "        gen_gradient = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "        optimizer_G.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"epoch {epoch}:\\nDiscriminator loss: {disc_loss.numpy()}\\nGenerator loss: {gen_loss.numpy()}\")\n",
    "        save_results(generator, epoch, noise)\n",
    "        \n",
    "\n",
    "generator.save(\"gen_model.h5\")\n",
    "discriminator.save(\"disc_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
